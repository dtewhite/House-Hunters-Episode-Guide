{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import nbimporter\n",
    "from function import combine_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 18]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that there were a few pages that were not properly scraped from HGTV. Here we check what seasons are missing.\n",
    "\n",
    "og_scraped_df = pd.read_csv('Resources/House_Hunters_International_Seasons_Grouped.csv', index_col = 0)\n",
    "missing_seasons = [ x for x in  np.arange(1,og_scraped_df.iloc[-1,0]) if x not in og_scraped_df['Season'].tolist() ]\n",
    "missing_seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While reviewing the data, I also noticed a few seasons only had 1 episode, so we will want to rescrape them too. The last \n",
    "# several seasons correctly only have 1 episode, so we exclude them.\n",
    "\n",
    "missing_seasons = missing_seasons + og_scraped_df.loc[og_scraped_df['Episode'] < 3, :].iloc[:-6,:]['Season'].tolist()\n",
    "missing_seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears the construction of the seasons' URLs changed over time for some reason. As such, we will need to filter the \n",
    "# seasons by format and concatenate several lists together. Season 1 actually had 2 seperate pages (1a and 100).\n",
    "\n",
    "rescrape_urls = [ f'https://www.hgtv.com/shows/house-hunters-international/episodes/{x}a' for x in missing_seasons if x == 1 ]\\\n",
    "    + [ f'https://www.hgtv.com/shows/house-hunters-international/episodes/{x}00' for x in missing_seasons if x < 145 ]\\\n",
    "    + [ f'https://www.hgtv.com/shows/house-hunters-international/episodes/{x}' for x in missing_seasons if x > 144 ]\n",
    "rescrape_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we rescrape the several pages that were determined to have errors.\n",
    "\n",
    "number_list = [] \n",
    "url_list = []\n",
    "title_list = []\n",
    "description_list = []\n",
    "\n",
    "for url in rescrape_urls:\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    episodes = soup.find('div', id = 'mod-episode-list-1')\n",
    "\n",
    "    episode_number = episodes.find_all('span', class_='m-EpisodeCard__a-AssetInfo')\n",
    "    episode_url = episodes.find_all('a')\n",
    "    episode_description = episodes.find_all('p', class_='m-EpisodeCard__a-Description')\n",
    "\n",
    "    numbers, urls, titles, descriptions = combine_data(episode_number, episode_url, episode_description)\n",
    "\n",
    "    number_list = number_list + numbers\n",
    "    url_list = url_list + urls\n",
    "    title_list = title_list + titles\n",
    "    description_list = description_list + descriptions\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a dictionary and then a Data Frame from the 4 lists returned by our web scraper.\n",
    "\n",
    "episode_dict = {}\n",
    "episode_dict['Number'] = number_list\n",
    "episode_dict['Title'] = title_list\n",
    "episode_dict['Description'] = description_list\n",
    "episode_dict['Link'] = url_list\n",
    "\n",
    "episodes_df = pd.DataFrame(episode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data cleaning to remove duplicates and seperate the 'Number' column into 'Season' and 'Episode'.\n",
    "\n",
    "episodes_df.drop_duplicates(subset = ['Title'])\n",
    "\n",
    "episodes_df[['Season','Episode']] = episodes_df['Number'].str.split(', ', expand = True)\n",
    "\n",
    "episodes_df['Season'] = episodes_df['Season'].map(lambda x:x.replace('Season ', ''))\n",
    "episodes_df['Episode'] = episodes_df['Episode'].map(lambda x:x.replace('Episode ', '')).astype(int)\n",
    "episodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_df.to_csv('Resources/House_Hunters_International_Rescraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
